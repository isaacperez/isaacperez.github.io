---
title: TinyGPT
date: 2024-08-20 12:00
categories: [General, Personal]
tags: [Projects]
---
# Introducing TinyGPT: A Simple, Educational Deep Learning Library
Introducing __TinyGPT__—an open-source, Python-based library designed to implement a simplified version of GPT with zero dependencies. TinyGPT is your gateway to understanding the inner workings of deep learning frameworks. Whether you're an experienced developer or just beginning your journey into AI, TinyGPT offers a hands-on approach to exploring, learning, and contributing to the world of machine learning.

<div style="text-align: center;">
  <img src="/assets/img/logo.svg" alt="TinyGPT Logo" style="display: inline-block; margin: 0 auto;"/>
<p>
  <a href="https://github.com/isaacperez/tinygpt">Visit the TinyGPT GitHub Repository</a>
</p>
</div>

Discover how deep learning works from the inside out with TinyGPT. Together, we can make complex concepts more accessible and build a community of learners who are excited to explore the details of AI.

---
## The Complexity of Modern Deep Learning Frameworks
Deep learning frameworks like [__PyTorch__](https://pytorch.org/), [__TensorFlow__](https://www.tensorflow.org/), and [__JAX__](https://github.com/google/jax) have revolutionized the field of machine learning, providing powerful tools to implement complex models with ease. However, as anyone who has ventured into their codebases knows, these libraries are also massive and intricate. As a learner, understanding how specific operations are implemented within these frameworks can be daunting. With tens of thousands of lines of code, layers of abstractions, and optimizations, tracing a simple operation or following the gradient flow can feel like getting lost in a maze.

## A Case in Point: PyTorch's Autograd
Take, for instance, PyTorch's autograd system, which is responsible for automatically calculating the gradients required for backpropagation. At a high level, using autograd is straightforward—just set `requires_grad=True` on your tensors, perform operations on them, and then call `.backward()` to compute gradients. But what happens under the hood?

When you dig into the PyTorch source code to understand how autograd works, you quickly find yourself navigating a labyrinth of classes, methods, and C++ code (since much of PyTorch is implemented in a mix of Python and C++ for efficiency). Understanding how a single operation is recorded in the computational graph involves tracing through multiple layers of abstraction:

  - __Tensors and Variables__: In the early versions of PyTorch, __Tensor__ and __Variable__ were separate classes, with Variable being responsible for holding the gradient information. In modern PyTorch, these have been merged, but the historical separation adds to the complexity when looking at older codebases or understanding the evolution of the framework.

  - __Function and Node Classes__: Each operation in PyTorch is represented by a __Function__ that records how the operation should be differentiated. These functions are connected into a graph of __Node__ objects, which form the computational graph. However, understanding how these nodes interact requires digging through multiple classes and methods spread across different files.

  - __C++ and CUDA Backend__: For performance reasons, many operations are actually implemented in C++ and CUDA, and then exposed to Python via bindings. This means that understanding the full implementation of autograd might require reading through C++ and CUDA code, which is not always straightforward, especially if you're not familiar with these languages.

Navigating this code to understand a single concept like backpropagation can take hours, if not days, of investigation. Even with extensive documentation, the complexity of the system can be overwhelming, particularly for beginners or those trying to learn how deep learning works at a lower level.

## Why TinyGPT?
With over a decade of experience studying and working in the field of deep learning, I've had the opportunity to explore some of the most powerful and complex frameworks in the industry. From PyTorch to TensorFlow, these tools have become essential for developing cutting-edge AI models. However, as I delved deeper into their inner workings, I realized just how challenging it can be to truly understand how they function beneath the surface.

TinyGPT is my attempt to distill the core ideas behind these powerful frameworks into a simplified and educational library. It’s designed for learners, hobbyists, and aspiring engineers who want to peel back the curtain and see how deep learning frameworks operate under the hood.

Many of us are familiar with using libraries like PyTorch, but when it comes to truly understanding how they implement backpropagation, autograd, or optimizers, things can get confusing fast. That’s because these libraries are built for production, prioritizing efficiency and scalability over readability. Their code has been honed over the years by thousands of developers, and while it’s robust and performant, it’s not always accessible for those who just want to learn.

TinyGPT, on the other hand, aims to be different. It’s deliberately minimal and focuses on implementing the core features of deep learning in a way that’s transparent and easy to follow. My goal was to create a library that explains itself—a place where each class, function, and line of code teaches you something about how deep learning works at a fundamental level.

In creating TinyGPT, I've drawn from my years of experience and my passion for making complex ideas accessible. I wanted to build a tool that not only works but also educates, helping others to grasp the essential concepts that power the AI systems we rely on today.

## Learning by Building
To build TinyGPT, I’ve absorbed lessons from some of the best minds in AI, from the streams of [__George Hotz__](https://www.youtube.com/watch?v=QXRXyVNltXY) to the lessons of [__Andrej Karpathy__](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=1). I’ve also drawn inspiration from open-source projects like [__Tinygrad__](https://github.com/tinygrad/tinygrad) and [__MLX__](https://github.com/ml-explore/mlx). These projects take an educational approach to AI, offering a more digestible look at the essential mechanics of deep learning. TinyGPT was born from the same spirit.

Here are some of the key features of TinyGPT:

  - __Educational__: TinyGPT is not designed to be the fastest or the most optimized deep learning library. Instead, its code is written to be as understandable as possible. Everything is explicit, with no black boxes, so you can trace the data flow, understand how gradients are calculated, and see how optimizers work.

  - __Simplicity First__: While large frameworks abstract many low-level details, TinyGPT intentionally avoids unnecessary abstraction. If you’re learning how backpropagation works, you won’t have to dig through layers of code just to find the logic that calculates gradients—everything is right there.

  - __Core Components__: TinyGPT includes core features such as autograd, backpropagation, tensor operations, and optimizers. While these concepts are implemented in much larger projects like PyTorch and TensorFlow, they are written in a more compact and easily digestible form here.

  - __Hands-on Learning__: TinyGPT is built to help you learn by doing. The implementation of Tensors, computational graphs, and even gradient descent is meant to be instructive. You can use TinyGPT to implement models while also understanding each step of the process.

## My Journey
TinyGPT is the result of my own journey through the complex world of machine learning frameworks. Like many, I started by using high-level libraries to train models, but I quickly realized that understanding how things worked under the surface was essential to becoming a better engineer. I wanted to break down the walls of abstraction, so I dove into the source code of PyTorch, TensorFlow, and other frameworks to see what was really going on.

Along the way, I found inspiration in the work of others who have taken this path before me. George Hotz’s live streams, where he builds deep learning frameworks from scratch, were a huge source of motivation. Similarly, Andrej Karpathy’s teachings on neural networks and AI, with his emphasis on making things understandable, guided much of the philosophy behind TinyGPT.

It’s through this journey that I realized the need for a more educational tool—something that didn’t just provide functions to train models but instead walked you through how those functions work. I wanted to build something that others could learn from, just as I learned from the codebases of the giants in this field.

# Conclusion
Deep learning frameworks can be intimidating, but they don’t have to be. TinyGPT is my attempt to show that the core ideas behind these frameworks can be made simple and educational. By breaking down complex operations into digestible pieces, I hope to help others build a deeper understanding of the foundations of deep learning.

TinyGPT is not just a tool for training models; it’s a tool for learning, exploring, and understanding. I invite you to dive in, play with the code, and see for yourself how deep learning frameworks work from the inside out.

---
<span style="color: gray;"> 
_**Disclaimer**: The content and redaction of this blog post were developed with the assistance of GPT-4. The ideas and experiences here are my own, but the writing process was enhanced through collaboration with AI to ensure clarity and engagement._